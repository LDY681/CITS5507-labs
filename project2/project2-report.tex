% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={CITS5507 Project 2: Parallel Implementation and Discovery with different settings},
  pdfauthor={Dayu Liu (24188516)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{CITS5507 Project 2: Parallel Implementation and Discovery with
different settings}
\author{Dayu Liu (24188516)}
\date{Semester 2, 2024}

\begin{document}
\maketitle

\section{Introduction}\label{introduction}

Matrix multiplication becomes increasingly expensive as the dimension
size grows, particularly with sparse matrices, which contain mostly zero
values and only a few non-zero elements. This results in unnecessary
computations involving zeros, making the process inefficient in terms of
both time and memory. In this research, we implemented a customized,
simplified version of compressed matrix multiplication to address these
inefficiencies. In the last project, we have explored how OpenMP
enhances parallelization and improves the performance of compressed
matrix multiplication by experimenting with different thread counts and
parallelization strategies.

In Project 2, we aimed to integrate MPI on top of OpenMP and conducted a
series of experiments with different approaches (Sequential, Pure
OpenMP, Pure MPI, and MPI+OpenMP), along with various parameter
combinations (number of nodes, processes per node, and threads per
process). In the sequential experiments, we successfully determined
which matrix density could handle the largest matrix size within a
10-minute time constraint. However, due to resource limitations on the
Pawsey supercomputer, we later shifted our objective to focus on
evaluating which configurations could complete the task most efficiently
in the subsequent experiments.

All code implementations are included in the submitted zip file. Please
refer to the comments in the \texttt{project2.sh} for instructions on
how to run the code. \vspace{0.5cm}

\section{Project Workflow}\label{project-workflow}

The project can be divided into two main parts:

\textbf{1. MPI Code Implementation:}\\
In the first step, we started with the code from project1, we integrated
implementations on MPI. The computing tasks are splitted evenly betwwen
processes by distributes an equal range of rows to compute to each
process. Barriers for collection are properly applied to make sure the
result is right. Some other key changes include now we don't write
matrices to files when generating, instead they are saved after the
experiment with \texttt{DEBUG} on to avoid taking account of time taken
with heavy load File I/O operations. We also get rid of some unused
functions on project 1's ordinary \texttt{matrixMultiply()}.

\textbf{2. Parallel Experiments:}\\
In the second step, we conducted experiments using varying node counts,
process counts, and thread counts. The performance was monitored using
Sequential, MPI, OpenMP, and MPI+OpenMP parallelization on the Pawsey
supercomputer. Performance was evaluated based on execution time across
different configurations. To conserve computing resources and address
the usage shortage, our experiments focused on several key
configurations rather than running them in bulk.

\vspace{0.5cm}

\section{Implementation}\label{implementation}

\subsection{Matrix Generation}\label{matrix-generation}

The \texttt{generateMatrices()} function is responsible for creating
matrices with a specified density of non-zero values. Each element in
the matrix is assigned a value based on a random selection process that
determines whether it will be a non-zero value, according to the
specified density (percent).

This function generates two compressed matrices (values and indices) row
by row. When a non-zero value is selected, a numerical value between 1
and 10 is assigned. These non-zero elements, along with their column
indices, are stored in two corresponding compressed matrices.

\subsection{Compressed Matrix Multiplication
Implementation}\label{compressed-matrix-multiplication-implementation}

The function \texttt{compressedMatrixMultiply()} leverages both MPI and
OpenMP to parallelize the multiplication of compressed matrices. It
divides the main loop into smaller sub-loops for each process, assigning
a specific range of rows between \texttt{start\_row} and
\texttt{end\_row} to each process. These sub-loops are then parallelized
using OpenMP's \texttt{parallel\ for} directive, which distributes the
work among a given number of threads.

For each element in the compressed matrix, its value and corresponding
index are extracted. These values are then used to compute the product,
and the result is added directly to the appropriate position in the
result matrix.

\subsection{MPI and OpenMP flags}\label{mpi-and-openmp-flags}

To make sure we can apply different combinations (Sequential, Pure
OpenMP, Pure MPI and MPI+OpenMP) in a unified code base, flags for MPI
\texttt{\_MPI} and OpenMP \texttt{\_OPENMP} are used to check if either
are enabled, this is done by adding \texttt{-fopenmp} and customized
\texttt{-D\_MPI} flags when compiling the program, a hybrid combination
would look like this:
\texttt{mpicxx\ -fopenmp\ -D\_MPI\ -o\ project2\ project2.c}. After that
we would use these flags to decide whether we need to initialize and
utilize MPI/OpenMP functionalities during computation.

\subsection{Message broadcasting and
barrier}\label{message-broadcasting-and-barrier}

Process 0 is responsible for generating the compressed matrices. After
the generation is complete, the resulting matrices are flattened into
1-D arrays using the \texttt{flattenCompressedMatrix()} function, and
the sizes of each row are recorded for MPI broadcasting. The data is
then broadcast to all other processes using \texttt{MPI\_Bcast()}. Each
process receives the flattened array and reconstructs the original
matrix using the \texttt{reconstructCompressedMatrix()} function,
ensuring that all processes are working with the same data for parallel
computation.

At the end of \texttt{compressedMatrixMultiply()}, a MPI barrier is
applied to ensure that all processes have completed their computations.
Once this synchronization is achieved, each process uses
\texttt{MPI\_Send()} to send its computed results to process 0. Process
0 then gathers these results using \texttt{MPI\_Recv()}, consolidating
the data to finalize the result.

\vspace{0.5cm}

\section{Experiments}\label{experiments}

\subsection{Experiment Setup}\label{experiment-setup}

The following parameters were explored during the experiments:

\textbf{Experiment 1: Time estimation based on Matrix size and Non-zero
Density}

\begin{itemize}
  \item Maximum Time allowed: 10 minutes
  \item Percentage of non-zero values in matrices: 1%, 2%, and 5%
\end{itemize}

In the first experiment, we would like to determine the impact of matrix
size and non-zero element density (1\%, 2\%, and 5\%) on execution time.
Due to limited resources, the experiment was executed sequentially,
though the insights gained should be applicable to other parallel
configurations as well.

\textbf{Experiment 2: Combination Comparison between different
parallelization}

\begin{itemize}
  \item Matrix dimension size: 100,000 x 100,000
  \item Parallel Combination: Pure MPI, Pure OpenMP, MPI+OpenMP
\end{itemize}

In the second experiment, the matrix with 1\% non-zero elements was used
to explore the impact of different parallel combinations. The goal was
to fully utilize all 128 physical cores per node for optimal
performance. In the pure MPI experiment, 128 processes per node were
created. In the pure OpenMP experiment, 128 threads per process were
used. For the MPI+OpenMP experiment, we allocated 4 MPI processes per
node, with 32 OpenMP threads per process.

\subsection{Experiment 1}\label{experiment-1}

In the batch file (\texttt{project2.sh}), we started with a 10,000 x
10,000 matrix and doubled the size in each iteration to find a rough
estimate of the largest matrix size that could be computed within 10
minutes. Once a rough estimate was obtained, we fine-tuned by
incrementing the matrix size by 1000 in subsequent iterations to arrive
at a more precise maximum size.

The executable is compiled once and reused across multiple iterations.
An argument flag \texttt{-\/-time=00:10:00} is applied to \texttt{srun}
to ensure the program terminates if it exceeds the 10-minute time limit.
Once the program has been terminated, we identified the last successful
iteration and recorded its corresponding matrix size.

\subsection{Experiment 1 Result}\label{experiment-1-result}

For the 1\% non-zero matrix, the maximum matrix size computed within the
10-minute constraint was 68,000 x 68,000, taking 588.622 seconds. For
the 2\% non-zero matrix, the maximum size dropped significantly to
46,000 x 46,000, taking 588.921 seconds, which is roughly 1/2.18 of the
1\% matrix size. For the 5\% non-zero matrix, the maximum size was
reduced further to 26,000 x 26,000 (1/6.84 compared to the 1\% matrix),
taking 597.242 seconds.

The significant decline in matrix size can be attributed to the
increased computational workload as matrix density rises. At 1\%
density, the abundance of zero elements allows the algorithm to bypass
many operations, focusing only on non-zero values. However, as density
increases to 2\% and 5\%, the proportion of non-zero elements grows,
leaving fewer opportunities to skip computations. This leads to a surge
in the number of necessary multiplications and additions, which directly
impacts performance, resulting in longer processing times for denser
matrices.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Non-zero Density & Time Elapsed (s) & Matrix Size Achieved \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1\% & 588.622 & 68,000 x 68,000 \\
2\% & 588.921 & 46,000 x 46,000 \\
5\% & 597.242 & 26,000 x 26,000 \\
\end{longtable}

\begin{quote}
Some of the graphs below use logarithmic scales or uneven breaks on the
X/Y axis to normalise the visual representation for easier comparison
over a wide range of values.
\end{quote}

\includegraphics{project2-report_files/figure-latex/unnamed-chunk-2-1.pdf}
However, for the larger 2\% and 5\% non-zero matrices, optimal
performance was achieved with 116-120 threads, the highest thread count
in our experiments. The increased density of non-zero elements results
in a larger computational workload, which can be more efficiently
parallelized across more threads, reducing the overall time as more
threads can handle the larger task in parallel.

Another plausible explanation is that for larger matrices like the 2\%
and 5\% cases, the increased memory demands can hinder performance
gains, thus requiring a higher thread count to achieve optimal
performance. To address these memory-bound limitations, reducing memory
consumption by using smaller data types, such as representing numerical
values from 0-10 with char instead of int, might shift the local minimum
execution time to a lower thread count.

\includegraphics{project2-report_files/figure-latex/unnamed-chunk-3-1.pdf}
\includegraphics{project2-report_files/figure-latex/unnamed-chunk-3-2.pdf}
\includegraphics{project2-report_files/figure-latex/unnamed-chunk-3-3.pdf}

\subsection{Experiment 2}\label{experiment-2}

In the main program (\texttt{project1.c}), command-line arguments divide
the workload by specifying different thread ranges and sparsity
percentages for the compressed matrix multiplication task. This enables
us to break the task into sub-tasks, each responsible for a specific
range of threads on a certain set of matrices.

Matrices are first loaded from pre-generated files using the
\texttt{loadMatrices()} function, which reads in compressed values and
indices, to ensure every sub-task is working on the same set of data.

Once the matrices are loaded, the \texttt{compressedMatrixMultiply()}
function is executed across varying thread counts, with OpenMP used to
parallelize the operation. The execution time is recorded for each
thread count using \texttt{omp\_get\_wtime()}, allowing us to evaluate
the performance at different levels of parallelization.

Experiment 2 tested how different OpenMP scheduling strategies (static,
dynamic, guided, and runtime) impact performance in matrix
multiplication. The 1\% matrix was selected because it provided optimal
performance at 60 threads in previous experiments.

Given the large matrix size (100,000 x 100,000), chunk size likely plays
a significant role in the efficiency of each scheduling type. Therefore,
additional experiments were conducted with chunk sizes of 100, 200, and
500, which seemed suitable for parallelization on 60 threads. The
default chunk size, indicated by 0, served as a baseline reference.

For runtime scheduling, which relies on the \texttt{OMP\_SCHEDULE}
environment variable, we couldn't set its parameter explicitly, but it
still provides a valuable comparison between different combinations.

\subsection{Experiment 2 result}\label{experiment-2-result}

At chunk size 0, which represents the default chunk size, the four
scheduling strategies demonstrated distinct performance patterns. Static
scheduling performed the worst among all, as its sensitivity to load
imbalances caused slower performance compared to more adaptive
strategies. Dynamic scheduling achieved the best performance, as it
effectively minimized idle time by distributing smaller chunks of work
to threads and ensuring balanced workloads. Runtime scheduling was the
second fastest, likely due to the environment variables being
well-optimized on Pawsey's system. Guided scheduling, while not as fast
as other candidates, performed slightly better than static, maintaining
a rather balanced workload distribution.

\begin{quote}
The static scheduling task with the default chunk size took longer in
Experiment 2 compared to Experiment 1. This can be attributed to
variations in individual node performance and Pawsey's slower transfer
speeds during the experiment on September 18th. To account for these
inconsistencies, we reran the static scheduling with the default chunk
size in Experiment 2 for a more reliable comparison.
\end{quote}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Scheduling Type & Time w/o Chunk size \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
static & 113.135 \\
dynamic & 61.9773 \\
guided & 101.167 \\
runtime & 89.1588 \\
\end{longtable}

\includegraphics{project2-report_files/figure-latex/unnamed-chunk-4-1.pdf}

In our side quest, we observed that the benefits of a more adaptive
strategy diminished as chunk sizes increased. In the dynamic strategy,
performance dropped significantly with larger chunk sizes due to the
inability to evenly distribute the workload across threads. The guided
strategy, however, performed better with a chunk size of 100 compared to
the default, suggesting that the default chunk size may not be
well-suited for larger workloads like this.

\includegraphics{project2-report_files/figure-latex/unnamed-chunk-5-1.pdf}

\end{document}
